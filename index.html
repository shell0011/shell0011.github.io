<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="UniTalking: A Unified Audio-Video Framework for Talking Portrait Generation">
  <title>UniTalking Project</title>
</head>

<!-- <body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://omnihuman-lab.github.io/v1_5/">
                      OmniHuman-1.5
                    </a>
                    <a class="navbar-item" href="https://omnihuman-lab.github.io/">
                      OmniHuman-1
                  </a>
                  <a class="navbar-item" href="https://zhenzhiwang.github.io/interacthuman/">
                      InterActHuman
                  </a>
                  <a class="navbar-item" href="https://alignhuman.github.io/">
                      AlignHuman
                  </a>
                    <a class="navbar-item" href="https://loopyavatar.github.io/">
                        Loopy
                    </a>
                    <a class="navbar-item" href="https://cyberhost.github.io/">
                        CyberHost
                    </a>
                    
                </div>
            </div>
        </div>

    </div>
  </nav> -->

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> UniTalking: A Unified Audio-Video Framework for Talking Portrait Generation </h1>
      <span>
        Hebeizi&nbsp;Li<sup></sup>,
        Benyuan&nbsp;Sun<sup>†</sup>,
        Yi&nbsp;Yang<sup></sup>,
        Zihao&nbsp;Liang<sup></sup>,
        Zihao&nbsp;Yin<sup></sup>,
        Xiao&nbsp;Sha<sup></sup>,
        Chenliang&nbsp;Wang<sup></sup>
        
        <br />
      </span>
      <span>Central Media Technology Institute, Huawei</span>
      <span><sup>†</sup>Project lead
        <!-- ,<sup>‡</sup>Internship at Bytedance -->
      </span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <a target="_blank" href="http://arxiv.org/abs/2502.01061" ><button>Paper</button></a>
<!-- 	      <a target="_blank" href="" onclick="alert('Coming Soon!');return false;"><button>Paper</button></a> -->
        <a target="_blank" href="https://omnihuman-lab.github.io"><button>Page</button></a>
      </div>
      <small><span><b>Abstract</b>: While state-of-the-art audio-video generation models like Veo3 and Sora2 demonstrate remarkable capabilities, their closed-source nature makes their architectures and training paradigms inaccessible. To bridge this gap in accessibility and performance, we introduce UniTalking, a unified, end-to-end diffusion framework for generating high-fidelity speech and lip-synchronized video. At its core, our framework employs Multi-Modal Transformer Blocks to explicitly model the fine-grained temporal correspondence between audio and video latent tokens via a shared self-attention mechanism. By leveraging powerful priors from a pre-trained video generation model, our framework ensures state-of-the-art visual fidelity while enabling efficient training. Furthermore, UniTalking incorporates a personalized voice cloning capability, allowing the generation of speech in a target style from a brief audio reference. Qualitative and quantitative results demonstrate that our method produces highly realistic talking portraits, achieving superior performance over existing open-source approaches in lip-sync accuracy, audio naturalness, and overall perceptual quality.</span></small>
      <div class='responsive-image-container'>
        <img src='image/fig-main.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      <h3>Text&Image to Audio&Video</h3>
        <!-- <p>
          OmniHuman supports various visual and audio styles. It can generate realistic human videos <strong>at any aspect ratio and body proportion (portrait, half-body, full-body all in one)</strong>, with realism stemming from comprehensive aspects including motion, lighting, and texture details.<br/>
          
        </p>
        <p class="styled-text">
          <b>*</b> Note that to generate all results on this page, <strong>only any single image and audio are required</strong>, except for the demo showcasing video and combined driving signals. For the sake of a clean layout, we have omitted the display of reference images, which are the first frame of the generated video in most cases. If you need comparisons or further information, please do not hesitate to contact us.
        </p> -->
        <div class="video-slider">
          <video src="video/main1.mp4"></video>
          <video src="video/main2.mp4"></video>
          <video src="video/main3.mp4"></video>
        </div>
        <div class="video-slider">
          <video src="video/talk2.mp4"></video>
          <video src="video/talk3.mp4"></video>
        </div>
        
        <!-- 
      <h3>Talking</h3>
        <p>OmniHuman can support input of any aspect ratio in terms of speech. It significantly improves the handling of gestures, which is a challenge for existing methods, and produces highly realistic results. The audio and images for some of the test cases are sourced from <a href="https://www.youtube.com/watch?v=5Jk8qITsqdM&t=127s&ab_channel=TEDxTalks">link1</a>, <a href="https://www.youtube.com/watch?v=ITxWUu6UcWQ&t=251s&ab_channel=TEDxTalks">link2</a>, <a href="https://www.youtube.com/watch?v=oO8w6XcXJUs&ab_channel=RealTimewithBillMaher">link3</a>, <a href="https://www.youtube.com/watch?v=oO8w6XcXJUs&ab_channel=RealTimewithBillMaher">link4</a>.</p>
        <div class="video-slider">
          <video src="video/talk2.mp4"></video>
          <video src="video/talk3.mp4"></video>
        </div>
        
 -->


      <h3>Text&Reference to Audio&Video</h3>
        <p><b>One-Ref</b></p>
        <div class="audio-single" style="margin-bottom: 1.5rem;">   <!-- 添加了简单下边距与第二行视频隔开，无背景、行距等额外样式 -->
          <audio controls src="audio/ref.mp3">
            您的浏览器不支持 audio 元素。
          </audio>
        </div>
        <div class="video-slider">
          <video src="video/one_ref_1.mp4"></video>
          <video src="video/one_ref_2.mp4"></video>
          <video src="video/one_ref_3.mp4"></video>
        </div>

        <p><b>Multi-Ref</b></p>
        <div style="display: flex; flex-direction: column; gap: 1.5rem; margin: 1rem 0;">
          
          <!-- 第1组：音频1 + 视频1 -->
          <div style="display: flex; gap: 2rem; align-items: center;">
            <div style="flex: 1;">
              <audio controls src="audio/ref_1.wav" style="width: 100%;">
                您的浏览器不支持 audio 元素。
              </audio>
            </div>
            <div style="flex: 1;">
              <video controls src="video/multi_ref_1.mp4" style="width: 100%;">
                您的浏览器不支持 video 元素。
              </video>
            </div>
          </div>

          <!-- 第2组：音频2 + 视频2 -->
          <div style="display: flex; gap: 2rem; align-items: center;">
            <div style="flex: 1;">
              <audio controls src="audio/ref_2.wav" style="width: 100%;">
                您的浏览器不支持 audio 元素。
              </audio>
            </div>
            <div style="flex: 1;">
              <video controls src="video/multi_ref_2.mp4" style="width: 100%;">
                您的浏览器不支持 video 元素。
              </video>
            </div>
          </div>

          <!-- 第3组：音频3 + 视频3 -->
          <div style="display: flex; gap: 2rem; align-items: center;">
            <div style="flex: 1;">
              <audio controls src="audio/ref_3.wav" style="width: 100%;">
                您的浏览器不支持 audio 元素。
              </audio>
            </div>
            <div style="flex: 1;">
              <video controls src="video/multi_ref_3.mp4" style="width: 100%;">
                您的浏览器不支持 video 元素。
              </video>
            </div>
          </div>

        </div>

      

      <h3>Ethics Concerns</h3>
        <p>
          The images and audios used in these demos are from public sources or generated by models, and are solely used to demonstrate the capabilities of this research work. If there are any concerns, please contact us (jianwen.alan@gmail.com) and we will delete it in time. The template of this webpage is based on the one from <a href="https://omnihuman-lab.github.io/">OmniHuman-1</a>.
        </p>
      
      <h3>BibTeX</h3>
        <p>If you find this project useful for your research, you can cite us and check out our other related works:</p>
        <pre><code>
          @article{lin2025omnihuman1,
            title={OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models}, 
            author={Gaojie Lin and Jianwen Jiang and Jiaqi Yang and Zerong Zheng and Chao Liang},
            journal={arXiv preprint arXiv:2502.01061},
            year={2025}
          }
        </code></pre> 

      <br/>
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
